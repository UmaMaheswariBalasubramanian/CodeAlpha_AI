{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2020-12-01T13:36:21.660709Z",
     "iopub.status.busy": "2020-12-01T13:36:21.659988Z",
     "iopub.status.idle": "2020-12-01T13:36:21.667061Z",
     "shell.execute_reply": "2020-12-01T13:36:21.667735Z"
    },
    "papermill": {
     "duration": 0.029976,
     "end_time": "2020-12-01T13:36:21.667935",
     "exception": false,
     "start_time": "2020-12-01T13:36:21.637959",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/language-translation-englishfrench/eng_-french.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "execution": {
     "iopub.execute_input": "2020-12-01T13:36:21.703975Z",
     "iopub.status.busy": "2020-12-01T13:36:21.703228Z",
     "iopub.status.idle": "2020-12-01T13:36:26.464291Z",
     "shell.execute_reply": "2020-12-01T13:36:26.463044Z"
    },
    "papermill": {
     "duration": 4.780942,
     "end_time": "2020-12-01T13:36:26.464432",
     "exception": false,
     "start_time": "2020-12-01T13:36:21.683490",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model\n",
    "from keras.layers import GRU, Input, Dense, TimeDistributed, Activation, RepeatVector, Bidirectional\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import sparse_categorical_crossentropy\n",
    "from keras import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-01T13:36:26.499434Z",
     "iopub.status.busy": "2020-12-01T13:36:26.497975Z",
     "iopub.status.idle": "2020-12-01T13:36:26.814338Z",
     "shell.execute_reply": "2020-12-01T13:36:26.814838Z"
    },
    "papermill": {
     "duration": 0.336451,
     "end_time": "2020-12-01T13:36:26.814998",
     "exception": false,
     "start_time": "2020-12-01T13:36:26.478547",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>English words/sentences</th>\n",
       "      <th>French words/sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>175616</th>\n",
       "      <td>Top-down economics never works, said Obama. \"T...</td>\n",
       "      <td>« L'économie en partant du haut vers le bas, ç...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175617</th>\n",
       "      <td>A carbon footprint is the amount of carbon dio...</td>\n",
       "      <td>Une empreinte carbone est la somme de pollutio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175618</th>\n",
       "      <td>Death is something that we're often discourage...</td>\n",
       "      <td>La mort est une chose qu'on nous décourage sou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175619</th>\n",
       "      <td>Since there are usually multiple websites on a...</td>\n",
       "      <td>Puisqu'il y a de multiples sites web sur chaqu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175620</th>\n",
       "      <td>If someone who doesn't know your background sa...</td>\n",
       "      <td>Si quelqu'un qui ne connaît pas vos antécédent...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  English words/sentences  \\\n",
       "175616  Top-down economics never works, said Obama. \"T...   \n",
       "175617  A carbon footprint is the amount of carbon dio...   \n",
       "175618  Death is something that we're often discourage...   \n",
       "175619  Since there are usually multiple websites on a...   \n",
       "175620  If someone who doesn't know your background sa...   \n",
       "\n",
       "                                   French words/sentences  \n",
       "175616  « L'économie en partant du haut vers le bas, ç...  \n",
       "175617  Une empreinte carbone est la somme de pollutio...  \n",
       "175618  La mort est une chose qu'on nous décourage sou...  \n",
       "175619  Puisqu'il y a de multiples sites web sur chaqu...  \n",
       "175620  Si quelqu'un qui ne connaît pas vos antécédent...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"../input/language-translation-englishfrench/eng_-french.csv\")\n",
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-01T13:36:26.850003Z",
     "iopub.status.busy": "2020-12-01T13:36:26.849374Z",
     "iopub.status.idle": "2020-12-01T13:36:26.853998Z",
     "shell.execute_reply": "2020-12-01T13:36:26.853364Z"
    },
    "papermill": {
     "duration": 0.024007,
     "end_time": "2020-12-01T13:36:26.854099",
     "exception": false,
     "start_time": "2020-12-01T13:36:26.830092",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "english = data[\"English words/sentences\"]\n",
    "french = data[\"French words/sentences\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-01T13:36:26.889574Z",
     "iopub.status.busy": "2020-12-01T13:36:26.888943Z",
     "iopub.status.idle": "2020-12-01T13:38:43.032511Z",
     "shell.execute_reply": "2020-12-01T13:38:43.031868Z"
    },
    "papermill": {
     "duration": 136.163755,
     "end_time": "2020-12-01T13:38:43.032643",
     "exception": false,
     "start_time": "2020-12-01T13:36:26.868888",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "for i,text in enumerate(english):\n",
    "  stri = \"\"\n",
    "  txt = tokenizer.tokenize(text)\n",
    "  for j in txt:\n",
    "    j = j.lower()\n",
    "    stri = stri + j\n",
    "    stri = stri + \" \"\n",
    "  english[i] = stri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-01T13:38:43.068555Z",
     "iopub.status.busy": "2020-12-01T13:38:43.067715Z",
     "iopub.status.idle": "2020-12-01T13:41:22.382649Z",
     "shell.execute_reply": "2020-12-01T13:41:22.381868Z"
    },
    "papermill": {
     "duration": 159.334829,
     "end_time": "2020-12-01T13:41:22.382805",
     "exception": false,
     "start_time": "2020-12-01T13:38:43.047976",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "for i,text in enumerate(french):\n",
    "  stri = \"\"\n",
    "  txt = tokenizer.tokenize(text)\n",
    "  for j in txt:\n",
    "    j = j.lower()\n",
    "    stri = stri + j\n",
    "    stri = stri + \" \"\n",
    "  french[i] = stri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-01T13:41:22.419751Z",
     "iopub.status.busy": "2020-12-01T13:41:22.418985Z",
     "iopub.status.idle": "2020-12-01T13:41:22.430178Z",
     "shell.execute_reply": "2020-12-01T13:41:22.429596Z"
    },
    "papermill": {
     "duration": 0.032254,
     "end_time": "2020-12-01T13:41:22.430277",
     "exception": false,
     "start_time": "2020-12-01T13:41:22.398023",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      hi \n",
      "1     run \n",
      "2     run \n",
      "3     who \n",
      "4     wow \n",
      "5    fire \n",
      "6    help \n",
      "7    jump \n",
      "8    stop \n",
      "9    stop \n",
      "Name: English words/sentences, dtype: object\n",
      "0        salut \n",
      "1        cours \n",
      "2       courez \n",
      "3          qui \n",
      "4     ça alors \n",
      "5       au feu \n",
      "6     à l aide \n",
      "7        saute \n",
      "8    ça suffit \n",
      "9         stop \n",
      "Name: French words/sentences, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(english[0:10])\n",
    "print(french[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-01T13:41:22.519548Z",
     "iopub.status.busy": "2020-12-01T13:41:22.499029Z",
     "iopub.status.idle": "2020-12-01T13:41:22.944588Z",
     "shell.execute_reply": "2020-12-01T13:41:22.943831Z"
    },
    "papermill": {
     "duration": 0.499396,
     "end_time": "2020-12-01T13:41:22.944796",
     "exception": false,
     "start_time": "2020-12-01T13:41:22.445400",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "are you envious \t->\têtes vous envieux \n",
      "\n",
      "are you envious \t->\têtes vous envieuses \n",
      "\n",
      "are you envious \t->\têtes vous envieuse \n",
      "\n",
      "are you envious \t->\tes tu envieux \n",
      "\n",
      "are you envious \t->\tes tu envieuse \n",
      "\n",
      "are you envious \t->\tes tu jaloux \n",
      "\n",
      "are you envious \t->\tes tu jalouse \n",
      "\n",
      "are you envious \t->\têtes vous jaloux \n",
      "\n",
      "are you envious \t->\têtes vous jalouses \n",
      "\n",
      "are you envious \t->\têtes vous jalouse \n",
      "\n",
      "are you excited \t->\tes tu énervé \n",
      "\n",
      "are you excited \t->\têtes vous énervé \n",
      "\n",
      "are you excited \t->\têtes vous énervés \n",
      "\n",
      "are you excited \t->\têtes vous énervées \n",
      "\n",
      "are you guys ok \t->\tça va tout le monde \n",
      "\n",
      "are you healthy \t->\têtes vous en bonne santé \n",
      "\n",
      "are you in love \t->\tes tu amoureux \n",
      "\n",
      "are you injured \t->\têtes vous blessés \n",
      "\n",
      "are you injured \t->\têtes vous blessées \n",
      "\n",
      "are you injured \t->\têtes vous blessée \n",
      "\n",
      "are you injured \t->\tes tu blessé \n",
      "\n",
      "are you injured \t->\tes tu blessée \n",
      "\n",
      "are you injured \t->\têtes vous blessé \n",
      "\n",
      "are you insured \t->\têtes vous assuré \n",
      "\n",
      "are you insured \t->\tes tu assuré \n",
      "\n",
      "are you jealous \t->\tes tu jaloux \n",
      "\n",
      "are you jealous \t->\tes tu jalouse \n",
      "\n",
      "are you jealous \t->\têtes vous jaloux \n",
      "\n",
      "are you jealous \t->\têtes vous jalouses \n",
      "\n",
      "are you jealous \t->\têtes vous jalouse \n",
      "\n",
      "are you kidding \t->\tvous plaisantez \n",
      "\n",
      "are you kidding \t->\ttu plaisantes \n",
      "\n",
      "are you kidding \t->\ttu plaisantes \n",
      "\n",
      "are you kidding \t->\tvous plaisantez \n",
      "\n",
      "are you kidding \t->\tplaisantez vous \n",
      "\n",
      "are you kidding \t->\tplaisantes tu \n",
      "\n",
      "are you looking \t->\tregardes tu \n",
      "\n",
      "are you looking \t->\tregardez vous \n",
      "\n",
      "are you married \t->\têtes vous marié \n",
      "\n",
      "are you nervous \t->\têtes vous nerveux \n",
      "\n",
      "are you nervous \t->\têtes vous nerveuse \n",
      "\n",
      "are you nervous \t->\têtes vous nerveuses \n",
      "\n",
      "are you on dope \t->\tes tu drogué \n",
      "\n",
      "are you on dope \t->\tes tu droguée \n",
      "\n",
      "are you on dope \t->\têtes vous drogués \n",
      "\n",
      "are you on dope \t->\têtes vous droguées \n",
      "\n",
      "are you on dope \t->\têtes vous drogué \n",
      "\n",
      "are you on dope \t->\têtes vous droguée \n",
      "\n",
      "are you relaxed \t->\têtes vous détendu \n",
      "\n",
      "are you relaxed \t->\têtes vous détendues \n",
      "\n",
      "are you relaxed \t->\têtes vous détendus \n",
      "\n",
      "are you relaxed \t->\têtes vous détendue \n",
      "\n",
      "are you relaxed \t->\tes tu détendu \n",
      "\n",
      "are you relaxed \t->\tes tu détendue \n",
      "\n",
      "are you serious \t->\ttu es sérieux \n",
      "\n",
      "are you serious \t->\têtes vous sérieux \n",
      "\n",
      "are you serious \t->\tt es sérieuse \n",
      "\n",
      "are you serious \t->\tc est vrai ce que tu dis \n",
      "\n",
      "are you sincere \t->\tes tu sincère \n",
      "\n",
      "are you sincere \t->\têtes vous sincère \n",
      "\n",
      "are you sisters \t->\têtes vous sœurs \n",
      "\n",
      "are you smiling \t->\têtes vous en train de sourire \n",
      "\n",
      "are you smiling \t->\test ce que tu es en train de sourire \n",
      "\n",
      "are you thirsty \t->\tavez vous soif \n",
      "\n",
      "are you thirsty \t->\tas tu soif \n",
      "\n",
      "are you thirsty \t->\tas tu soif \n",
      "\n",
      "are you thirsty \t->\tavez vous soif \n",
      "\n",
      "are you unlucky \t->\tes tu malchanceux \n",
      "\n",
      "are you unlucky \t->\tes tu malchanceuse \n",
      "\n",
      "are you unlucky \t->\têtes vous malchanceux \n",
      "\n",
      "are you unlucky \t->\têtes vous malchanceuse \n",
      "\n",
      "are you unlucky \t->\têtes vous malchanceuses \n",
      "\n",
      "are you winning \t->\ttu gagnes \n",
      "\n",
      "are you winning \t->\test ce que vous êtes en train de gagner \n",
      "\n",
      "are you winning \t->\têtes vous en train de gagner \n",
      "\n",
      "are you with us \t->\tes tu avec nous \n",
      "\n",
      "are you working \t->\ttravaillez vous \n",
      "\n",
      "are you working \t->\tes tu en train de travailler \n",
      "\n",
      "are you working \t->\têtes vous en train de travailler \n",
      "\n",
      "aren t we lucky \t->\tn avons nous pas de chance \n",
      "\n",
      "aren t we lucky \t->\tne sommes nous pas chanceux \n",
      "\n",
      "aren t we lucky \t->\tne sommes nous pas chanceuses \n",
      "\n",
      "aren t you cold \t->\tn avez vous pas froid \n",
      "\n",
      "aren t you late \t->\tn es tu pas en retard \n",
      "\n",
      "aren t you late \t->\tn êtes vous pas en retard \n",
      "\n",
      "ask me anything \t->\tdemande moi n importe quoi \n",
      "\n",
      "ask me tomorrow \t->\tdemande moi demain \n",
      "\n",
      "ask me tomorrow \t->\tdemandez moi demain \n",
      "\n",
      "balls are round \t->\tles balles sont rondes \n",
      "\n",
      "be careful tom \t->\tfais attention tom \n",
      "\n",
      "be careful tom \t->\tfais attention tom \n",
      "\n",
      "be careful tom \t->\tsois prudent tom \n",
      "\n",
      "be more precise \t->\tsoyez plus précis \n",
      "\n",
      "be more precise \t->\tsoit plus précis \n",
      "\n",
      "be quiet girls \t->\trestez tranquilles les filles \n",
      "\n",
      "be very careful \t->\tsois très prudent \n",
      "\n",
      "be very careful \t->\tsoyez très prudent \n",
      "\n",
      "be very careful \t->\tsoyez très prudente \n",
      "\n",
      "be very careful \t->\tsoyez très prudents \n",
      "\n",
      "be very careful \t->\tsoyez très prudentes \n",
      "\n"
     ]
    }
   ],
   "source": [
    "n1 = 9900\n",
    "n2=10000\n",
    "eng = []\n",
    "fre = []\n",
    "\n",
    "for data in english:\n",
    "  eng.append(data)\n",
    "\n",
    "for data in french:\n",
    "  fre.append(data)\n",
    "\n",
    "eng = np.asarray(eng)\n",
    "fre = np.asarray(fre)\n",
    "\n",
    "eng = eng[0:175000]\n",
    "fre = fre[0:175000]\n",
    "\n",
    "for i in range(n1,n2):\n",
    "  print(eng[i] + \"\\t->\\t\" + fre[i] + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-01T13:41:22.995306Z",
     "iopub.status.busy": "2020-12-01T13:41:22.994420Z",
     "iopub.status.idle": "2020-12-01T13:41:22.997971Z",
     "shell.execute_reply": "2020-12-01T13:41:22.998459Z"
    },
    "papermill": {
     "duration": 0.027498,
     "end_time": "2020-12-01T13:41:22.998582",
     "exception": false,
     "start_time": "2020-12-01T13:41:22.971084",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Shape is (175000,)\n",
      "French Shape is (175000,)\n"
     ]
    }
   ],
   "source": [
    "print(\"English Shape is \" + str(eng.shape))\n",
    "print(\"French Shape is \" + str(fre.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-01T13:41:23.059090Z",
     "iopub.status.busy": "2020-12-01T13:41:23.038516Z",
     "iopub.status.idle": "2020-12-01T13:41:25.034866Z",
     "shell.execute_reply": "2020-12-01T13:41:25.036080Z"
    },
    "papermill": {
     "duration": 2.021471,
     "end_time": "2020-12-01T13:41:25.036267",
     "exception": false,
     "start_time": "2020-12-01T13:41:23.014796",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1133720 English words.\n",
      "1250733 French words.\n",
      "\n",
      "\n",
      "13917 unique English words.\n",
      "23918 unique French words.\n",
      "\n",
      "\n",
      "10 Most common words in the English dataset:\n",
      "\"\" \"i\" \"you\" \"to\" \"the\" \"a\" \"t\" \"is\" \"that\" \"tom\"\n",
      "\n",
      "\n",
      "10 Most common words in the French dataset:\n",
      "\"\" \"je\" \"de\" \"pas\" \"est\" \"vous\" \"que\" \"il\" \"à\" \"ne\"\n"
     ]
    }
   ],
   "source": [
    "english_word_counter = collections.Counter([word for sentence in eng for word in sentence.split(\" \")])\n",
    "french_word_counter = collections.Counter([word for sentence in fre for word in sentence.split(\" \")])\n",
    "\n",
    "print('{} English words.'.format(len([word for sentence in eng for word in sentence.split()])))\n",
    "print('{} French words.'.format(len([word for sentence in fre for word in sentence.split()])))\n",
    "print(\"\\n\")\n",
    "print('{} unique English words.'.format(len(english_word_counter)))\n",
    "print('{} unique French words.'.format(len(french_word_counter)))\n",
    "print(\"\\n\")\n",
    "print('10 Most common words in the English dataset:')\n",
    "print('\"' + '\" \"'.join(list(zip(*english_word_counter.most_common(10)))[0]) + '\"')\n",
    "print(\"\\n\")\n",
    "print('10 Most common words in the French dataset:')\n",
    "print('\"' + '\" \"'.join(list(zip(*french_word_counter.most_common(10)))[0]) + '\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-01T13:41:25.117188Z",
     "iopub.status.busy": "2020-12-01T13:41:25.116248Z",
     "iopub.status.idle": "2020-12-01T13:41:25.122329Z",
     "shell.execute_reply": "2020-12-01T13:41:25.123366Z"
    },
    "papermill": {
     "duration": 0.059586,
     "end_time": "2020-12-01T13:41:25.123526",
     "exception": false,
     "start_time": "2020-12-01T13:41:25.063940",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2, 3, 5, 6, 7, 8, 2, 9, 10], [11, 12, 13, 3, 14, 15, 16, 17, 4, 18], [19, 20, 4, 21, 22]]\n",
      "{' ': 1, 'the': 2, 'quick': 3, 'a': 4, 'brown': 5, 'fox': 6, 'jumps': 7, 'over': 8, 'lazy': 9, 'dog': 10, 'by': 11, 'jove': 12, 'my': 13, 'study': 14, 'of': 15, 'lexicography': 16, 'won': 17, 'prize': 18, 'this': 19, 'is': 20, 'short': 21, 'sentence': 22}\n"
     ]
    }
   ],
   "source": [
    "def tokenize(x):\n",
    "  tokenizer = Tokenizer(char_level=False,oov_token=\" \")\n",
    "  tokenizer.fit_on_texts(x)\n",
    "  return tokenizer.texts_to_sequences(x), tokenizer\n",
    "\n",
    "text_sentences = [\n",
    "    'The quick brown fox jumps over the lazy dog .',\n",
    "    'By Jove , my quick study of lexicography won a prize .',\n",
    "    'This is a short sentence .']\n",
    "  \n",
    "text , tokenizer = tokenize(text_sentences)\n",
    "print(text)\n",
    "print(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-01T13:41:25.219150Z",
     "iopub.status.busy": "2020-12-01T13:41:25.215838Z",
     "iopub.status.idle": "2020-12-01T13:41:25.222643Z",
     "shell.execute_reply": "2020-12-01T13:41:25.218491Z"
    },
    "papermill": {
     "duration": 0.073676,
     "end_time": "2020-12-01T13:41:25.222802",
     "exception": false,
     "start_time": "2020-12-01T13:41:25.149126",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence 1 in x\n",
      "  Input:  [ 2  3  5  6  7  8  2  9 10]\n",
      "  Output: [ 2  3  5  6  7  8  2  9 10  0]\n",
      "Sequence 2 in x\n",
      "  Input:  [11 12 13  3 14 15 16 17  4 18]\n",
      "  Output: [11 12 13  3 14 15 16 17  4 18]\n",
      "Sequence 3 in x\n",
      "  Input:  [19 20  4 21 22]\n",
      "  Output: [19 20  4 21 22  0  0  0  0  0]\n"
     ]
    }
   ],
   "source": [
    "def pad(x,length=None):\n",
    "  if (length==None):\n",
    "    length = max([len(sentence) for sentence in x])\n",
    "  a = pad_sequences(x,maxlen=length,padding=\"post\")\n",
    "  return a\n",
    "\n",
    "test_pad = pad(text)\n",
    "for i, (token_sent, pad_sent) in enumerate(zip(text, test_pad)):\n",
    "    print('Sequence {} in x'.format(i + 1))\n",
    "    print('  Input:  {}'.format(np.array(token_sent)))\n",
    "    print('  Output: {}'.format(pad_sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-01T13:41:25.292660Z",
     "iopub.status.busy": "2020-12-01T13:41:25.291937Z",
     "iopub.status.idle": "2020-12-01T13:41:25.295437Z",
     "shell.execute_reply": "2020-12-01T13:41:25.296109Z"
    },
    "papermill": {
     "duration": 0.041431,
     "end_time": "2020-12-01T13:41:25.296254",
     "exception": false,
     "start_time": "2020-12-01T13:41:25.254823",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess(x,y):\n",
    "  preprocess_x,x_tk = tokenize(x)\n",
    "  preprocess_y,y_tk = tokenize(y)\n",
    "\n",
    "  preprocess_x = pad(preprocess_x)\n",
    "  preprocess_y = pad(preprocess_y)\n",
    "  preprocess_y = preprocess_y.reshape(*preprocess_y.shape, 1)\n",
    "\n",
    "  return preprocess_x, preprocess_y, x_tk, y_tk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-01T13:41:25.357750Z",
     "iopub.status.busy": "2020-12-01T13:41:25.356948Z",
     "iopub.status.idle": "2020-12-01T13:41:40.238275Z",
     "shell.execute_reply": "2020-12-01T13:41:40.221536Z"
    },
    "papermill": {
     "duration": 14.917951,
     "end_time": "2020-12-01T13:41:40.238476",
     "exception": false,
     "start_time": "2020-12-01T13:41:25.320525",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Preprocessed\n",
      "Max English sentence length: 21\n",
      "Max French sentence length: 26\n",
      "English vocabulary size: 13917\n",
      "French vocabulary size: 23918\n"
     ]
    }
   ],
   "source": [
    "pre_eng,pre_fre,eng_tk,fre_tk = preprocess(eng,fre)\n",
    "max_eng_seq_len = pre_eng.shape[1]\n",
    "max_fr_seq_len = pre_fre.shape[1]\n",
    "english_vocab_size = len(eng_tk.word_index)\n",
    "french_vocab_size = len(fre_tk.word_index)\n",
    "\n",
    "print('Data Preprocessed')\n",
    "print(\"Max English sentence length:\", max_eng_seq_len)\n",
    "print(\"Max French sentence length:\", max_fr_seq_len)\n",
    "print(\"English vocabulary size:\", english_vocab_size)\n",
    "print(\"French vocabulary size:\", french_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-01T13:41:40.357621Z",
     "iopub.status.busy": "2020-12-01T13:41:40.356677Z",
     "iopub.status.idle": "2020-12-01T14:43:19.358839Z",
     "shell.execute_reply": "2020-12-01T14:43:19.359633Z"
    },
    "papermill": {
     "duration": 3699.074338,
     "end_time": "2020-12-01T14:43:19.359968",
     "exception": false,
     "start_time": "2020-12-01T13:41:40.285630",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 26, 128)           1781504   \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 26, 256)           198144    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 26, 256)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed (TimeDistri (None, 26, 23918)         6146926   \n",
      "=================================================================\n",
      "Total params: 8,126,574\n",
      "Trainable params: 8,126,574\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "137/137 [==============================] - 73s 535ms/step - loss: 2.8739 - accuracy: 0.7602 - val_loss: nan - val_accuracy: 0.5928\n",
      "Epoch 2/50\n",
      "137/137 [==============================] - 73s 533ms/step - loss: 1.5941 - accuracy: 0.7686 - val_loss: nan - val_accuracy: 0.5934\n",
      "Epoch 3/50\n",
      "137/137 [==============================] - 73s 533ms/step - loss: 1.5553 - accuracy: 0.7724 - val_loss: nan - val_accuracy: 0.6011\n",
      "Epoch 4/50\n",
      "137/137 [==============================] - 73s 533ms/step - loss: 1.4900 - accuracy: 0.7837 - val_loss: nan - val_accuracy: 0.6179\n",
      "Epoch 5/50\n",
      "137/137 [==============================] - 73s 534ms/step - loss: 1.3904 - accuracy: 0.7947 - val_loss: nan - val_accuracy: 0.6288\n",
      "Epoch 6/50\n",
      "137/137 [==============================] - 73s 534ms/step - loss: 1.2955 - accuracy: 0.8035 - val_loss: nan - val_accuracy: 0.6367\n",
      "Epoch 7/50\n",
      "137/137 [==============================] - 73s 534ms/step - loss: 1.2135 - accuracy: 0.8102 - val_loss: nan - val_accuracy: 0.6438\n",
      "Epoch 8/50\n",
      "137/137 [==============================] - 73s 531ms/step - loss: 1.1423 - accuracy: 0.8157 - val_loss: nan - val_accuracy: 0.6504\n",
      "Epoch 9/50\n",
      "137/137 [==============================] - 73s 535ms/step - loss: 1.0788 - accuracy: 0.8208 - val_loss: nan - val_accuracy: 0.6560\n",
      "Epoch 10/50\n",
      "137/137 [==============================] - 73s 535ms/step - loss: 1.0235 - accuracy: 0.8252 - val_loss: nan - val_accuracy: 0.6613\n",
      "Epoch 11/50\n",
      "137/137 [==============================] - 73s 535ms/step - loss: 0.9758 - accuracy: 0.8290 - val_loss: nan - val_accuracy: 0.6662\n",
      "Epoch 12/50\n",
      "137/137 [==============================] - 73s 535ms/step - loss: 0.9349 - accuracy: 0.8322 - val_loss: nan - val_accuracy: 0.6693\n",
      "Epoch 13/50\n",
      "137/137 [==============================] - 73s 532ms/step - loss: 0.8991 - accuracy: 0.8351 - val_loss: nan - val_accuracy: 0.6719\n",
      "Epoch 14/50\n",
      "137/137 [==============================] - 73s 536ms/step - loss: 0.8682 - accuracy: 0.8375 - val_loss: nan - val_accuracy: 0.6745\n",
      "Epoch 15/50\n",
      "137/137 [==============================] - 74s 537ms/step - loss: 0.8411 - accuracy: 0.8395 - val_loss: nan - val_accuracy: 0.6763\n",
      "Epoch 16/50\n",
      "137/137 [==============================] - 73s 533ms/step - loss: 0.8169 - accuracy: 0.8416 - val_loss: nan - val_accuracy: 0.6779\n",
      "Epoch 17/50\n",
      "137/137 [==============================] - 73s 534ms/step - loss: 0.7956 - accuracy: 0.8431 - val_loss: nan - val_accuracy: 0.6799\n",
      "Epoch 18/50\n",
      "137/137 [==============================] - 73s 536ms/step - loss: 0.7759 - accuracy: 0.8447 - val_loss: nan - val_accuracy: 0.6810\n",
      "Epoch 19/50\n",
      "137/137 [==============================] - 73s 533ms/step - loss: 0.7581 - accuracy: 0.8465 - val_loss: nan - val_accuracy: 0.6821\n",
      "Epoch 20/50\n",
      "137/137 [==============================] - 73s 535ms/step - loss: 0.7422 - accuracy: 0.8478 - val_loss: nan - val_accuracy: 0.6832\n",
      "Epoch 21/50\n",
      "137/137 [==============================] - 73s 536ms/step - loss: 0.7269 - accuracy: 0.8493 - val_loss: nan - val_accuracy: 0.6843\n",
      "Epoch 22/50\n",
      "137/137 [==============================] - 73s 536ms/step - loss: 0.7133 - accuracy: 0.8505 - val_loss: nan - val_accuracy: 0.6853\n",
      "Epoch 23/50\n",
      "137/137 [==============================] - 73s 535ms/step - loss: 0.7001 - accuracy: 0.8520 - val_loss: nan - val_accuracy: 0.6865\n",
      "Epoch 24/50\n",
      "137/137 [==============================] - 73s 536ms/step - loss: 0.6880 - accuracy: 0.8533 - val_loss: nan - val_accuracy: 0.6872\n",
      "Epoch 25/50\n",
      "137/137 [==============================] - 73s 534ms/step - loss: 0.6767 - accuracy: 0.8546 - val_loss: nan - val_accuracy: 0.6886\n",
      "Epoch 26/50\n",
      "137/137 [==============================] - 73s 535ms/step - loss: 0.6667 - accuracy: 0.8558 - val_loss: nan - val_accuracy: 0.6889\n",
      "Epoch 27/50\n",
      "137/137 [==============================] - 73s 535ms/step - loss: 0.6569 - accuracy: 0.8568 - val_loss: nan - val_accuracy: 0.6898\n",
      "Epoch 28/50\n",
      "137/137 [==============================] - 73s 535ms/step - loss: 0.6476 - accuracy: 0.8580 - val_loss: nan - val_accuracy: 0.6901\n",
      "Epoch 29/50\n",
      "137/137 [==============================] - 73s 535ms/step - loss: 0.6390 - accuracy: 0.8591 - val_loss: nan - val_accuracy: 0.6910\n",
      "Epoch 30/50\n",
      "137/137 [==============================] - 73s 536ms/step - loss: 0.6303 - accuracy: 0.8601 - val_loss: nan - val_accuracy: 0.6914\n",
      "Epoch 31/50\n",
      "137/137 [==============================] - 73s 536ms/step - loss: 0.6226 - accuracy: 0.8611 - val_loss: nan - val_accuracy: 0.6918\n",
      "Epoch 32/50\n",
      "137/137 [==============================] - 73s 534ms/step - loss: 0.6146 - accuracy: 0.8622 - val_loss: nan - val_accuracy: 0.6922\n",
      "Epoch 33/50\n",
      "137/137 [==============================] - 73s 535ms/step - loss: 0.6074 - accuracy: 0.8631 - val_loss: nan - val_accuracy: 0.6926\n",
      "Epoch 34/50\n",
      "137/137 [==============================] - 73s 536ms/step - loss: 0.6000 - accuracy: 0.8641 - val_loss: nan - val_accuracy: 0.6933\n",
      "Epoch 35/50\n",
      "137/137 [==============================] - 73s 536ms/step - loss: 0.5933 - accuracy: 0.8651 - val_loss: nan - val_accuracy: 0.6936\n",
      "Epoch 36/50\n",
      "137/137 [==============================] - 73s 536ms/step - loss: 0.5871 - accuracy: 0.8658 - val_loss: nan - val_accuracy: 0.6933\n",
      "Epoch 37/50\n",
      "137/137 [==============================] - 73s 535ms/step - loss: 0.5809 - accuracy: 0.8667 - val_loss: nan - val_accuracy: 0.6936\n",
      "Epoch 38/50\n",
      "137/137 [==============================] - 73s 531ms/step - loss: 0.5747 - accuracy: 0.8675 - val_loss: nan - val_accuracy: 0.6935\n",
      "Epoch 39/50\n",
      "137/137 [==============================] - 73s 532ms/step - loss: 0.5691 - accuracy: 0.8682 - val_loss: nan - val_accuracy: 0.6933\n",
      "Epoch 40/50\n",
      "137/137 [==============================] - 73s 534ms/step - loss: 0.5637 - accuracy: 0.8691 - val_loss: nan - val_accuracy: 0.6948\n",
      "Epoch 41/50\n",
      "137/137 [==============================] - 73s 536ms/step - loss: 0.5579 - accuracy: 0.8697 - val_loss: nan - val_accuracy: 0.6948\n",
      "Epoch 42/50\n",
      "137/137 [==============================] - 73s 535ms/step - loss: 0.5526 - accuracy: 0.8705 - val_loss: nan - val_accuracy: 0.6946\n",
      "Epoch 43/50\n",
      "137/137 [==============================] - 73s 534ms/step - loss: 0.5474 - accuracy: 0.8713 - val_loss: nan - val_accuracy: 0.6953\n",
      "Epoch 44/50\n",
      "137/137 [==============================] - 73s 531ms/step - loss: 0.5433 - accuracy: 0.8719 - val_loss: nan - val_accuracy: 0.6954\n",
      "Epoch 45/50\n",
      "137/137 [==============================] - 73s 535ms/step - loss: 0.5380 - accuracy: 0.8726 - val_loss: nan - val_accuracy: 0.6958\n",
      "Epoch 46/50\n",
      "137/137 [==============================] - 73s 536ms/step - loss: 0.5328 - accuracy: 0.8732 - val_loss: nan - val_accuracy: 0.6956\n",
      "Epoch 47/50\n",
      "137/137 [==============================] - 73s 535ms/step - loss: 0.5283 - accuracy: 0.8739 - val_loss: nan - val_accuracy: 0.6955\n",
      "Epoch 48/50\n",
      "137/137 [==============================] - 73s 534ms/step - loss: 0.5243 - accuracy: 0.8746 - val_loss: nan - val_accuracy: 0.6954\n",
      "Epoch 49/50\n",
      "137/137 [==============================] - 73s 534ms/step - loss: 0.5203 - accuracy: 0.8752 - val_loss: nan - val_accuracy: 0.6963\n",
      "Epoch 50/50\n",
      "137/137 [==============================] - 73s 535ms/step - loss: 0.5161 - accuracy: 0.8759 - val_loss: nan - val_accuracy: 0.6963\n"
     ]
    }
   ],
   "source": [
    "def embed_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
    "\n",
    "    learning_rate = 1e-3\n",
    "    model = keras.Sequential([\n",
    "        Embedding(english_vocab_size+1, 128, input_length=input_shape[1]),\n",
    "        Bidirectional(GRU(128, return_sequences=True)),\n",
    "        tf.keras.layers.Dropout(0.25),\n",
    "        TimeDistributed(Dense(french_vocab_size, activation='softmax'))\n",
    "    ])\n",
    "    model.summary()\n",
    "    \n",
    "    model.compile(loss=sparse_categorical_crossentropy,\n",
    "                  optimizer=Adam(learning_rate),\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "tmp_x = pad(pre_eng, max_fr_seq_len)\n",
    "rnn_model = embed_model(tmp_x.shape,max_fr_seq_len,english_vocab_size,french_vocab_size)\n",
    "rnn_model.fit(tmp_x, pre_fre, batch_size=1024, epochs=50, validation_split=0.2)\n",
    "rnn_model.save_weights(\"rnn_model_weights.h5\")\n",
    "#rnn_model.load_weights(\"/content/rnn_model_weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-01T14:43:24.724520Z",
     "iopub.status.busy": "2020-12-01T14:43:24.723614Z",
     "iopub.status.idle": "2020-12-01T14:43:24.726796Z",
     "shell.execute_reply": "2020-12-01T14:43:24.726278Z"
    },
    "papermill": {
     "duration": 2.681969,
     "end_time": "2020-12-01T14:43:24.726915",
     "exception": false,
     "start_time": "2020-12-01T14:43:22.044946",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def logits_to_text(logits, tokenizer):\n",
    "  index_to_words = {id: word for word, id in tokenizer.word_index.items()}\n",
    "  index_to_words[0] = '<PAD>'\n",
    "\n",
    "  return ' '.join([index_to_words[prediction] for prediction in np.argmax(logits, 1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-01T14:43:29.579484Z",
     "iopub.status.busy": "2020-12-01T14:43:29.578573Z",
     "iopub.status.idle": "2020-12-01T14:43:30.329887Z",
     "shell.execute_reply": "2020-12-01T14:43:30.330386Z"
    },
    "papermill": {
     "duration": 3.173017,
     "end_time": "2020-12-01T14:43:30.330526",
     "exception": false,
     "start_time": "2020-12-01T14:43:27.157509",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nous\n",
      "gaspillez\n",
      "timides\n",
      "<PAD>\n",
      "<PAD>\n",
      "[ 30  55 359   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0]\n",
      "be very careful \t->\tsois très prudente \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "  print(logits_to_text(rnn_model.predict(tmp_x[1000])[i], fre_tk))\n",
    "print(tmp_x[10000])\n",
    "\n",
    "i = 10000\n",
    "print(eng[i] + \"\\t->\\t\" + fre[i] + \"\\n\")\n",
    "\n",
    "\n",
    "#[24 ,80 ,159 ,0 ,  0 ,  0,   0,   0,   0,   0,   0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-01T14:43:35.343485Z",
     "iopub.status.busy": "2020-12-01T14:43:35.342561Z",
     "iopub.status.idle": "2020-12-01T14:43:36.515920Z",
     "shell.execute_reply": "2020-12-01T14:43:36.515199Z"
    },
    "papermill": {
     "duration": 3.808782,
     "end_time": "2020-12-01T14:43:36.516066",
     "exception": false,
     "start_time": "2020-12-01T14:43:32.707284",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fox \n"
     ]
    }
   ],
   "source": [
    "new_words = []\n",
    "num_arr = [0 for i in range(11)]\n",
    "sentence = \"is\"\n",
    "sentence = sentence.lower()\n",
    "words = sentence.split(\" \")\n",
    "\n",
    "for i,word in enumerate(words):\n",
    "  num_arr[i] = eng_tk.word_index[word]\n",
    "\n",
    "\n",
    "arra = []\n",
    "for i in range(10):\n",
    "  arra.append(logits_to_text(rnn_model.predict(num_arr)[i], fre_tk))\n",
    "\n",
    "sent = \"\"\n",
    "for word in arra:\n",
    "  if word == \"<PAD>\":\n",
    "    break\n",
    "  else:\n",
    "    sent = sent + word\n",
    "    sent = sent + \" \"\n",
    "\n",
    "print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 2.684234,
     "end_time": "2020-12-01T14:43:41.554585",
     "exception": false,
     "start_time": "2020-12-01T14:43:38.870351",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "papermill": {
   "duration": 4048.594564,
   "end_time": "2020-12-01T14:43:46.027469",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2020-12-01T13:36:17.432905",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
